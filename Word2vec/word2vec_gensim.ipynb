{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ad843e3-ac0b-4266-9eba-52eba01d739e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CMH\\.conda\\envs\\NLP\\Lib\\site-packages\\jieba\\_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "Building prefix dict from the default dictionary ...\n",
      "2025-06-23 16:10:20,372 : DEBUG : Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\CMH\\AppData\\Local\\Temp\\jieba.cache\n",
      "2025-06-23 16:10:20,374 : DEBUG : Loading model from cache C:\\Users\\CMH\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.053 seconds.\n",
      "2025-06-23 16:10:21,426 : DEBUG : Loading model cost 1.053 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "2025-06-23 16:10:21,427 : DEBUG : Prefix dict has been built successfully.\n",
      "2025-06-23 16:10:23,207 : INFO : collecting all words and their counts\n",
      "2025-06-23 16:10:23,208 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2025-06-23 16:10:23,254 : INFO : collected 12099 word types from a corpus of 188848 raw words and 10000 sentences\n",
      "2025-06-23 16:10:23,255 : INFO : Creating a fresh vocabulary\n",
      "2025-06-23 16:10:23,273 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=3 retains 4028 unique words (33.29% of original 12099, drops 8071)', 'datetime': '2025-06-23T16:10:23.273036', 'gensim': '4.3.3', 'python': '3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:29:09) [MSC v.1943 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-06-23 16:10:23,274 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=3 leaves 179103 word corpus (94.84% of original 188848, drops 9745)', 'datetime': '2025-06-23T16:10:23.273036', 'gensim': '4.3.3', 'python': '3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:29:09) [MSC v.1943 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-06-23 16:10:23,302 : INFO : deleting the raw counts dictionary of 12099 items\n",
      "2025-06-23 16:10:23,304 : INFO : sample=0.001 downsamples 47 most-common words\n",
      "2025-06-23 16:10:23,305 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 127397.55238210877 word corpus (71.1%% of prior 179103)', 'datetime': '2025-06-23T16:10:23.305050', 'gensim': '4.3.3', 'python': '3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:29:09) [MSC v.1943 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'prepare_vocab'}\n",
      "2025-06-23 16:10:23,347 : INFO : estimated required memory for 4028 words and 300 dimensions: 11681200 bytes\n",
      "2025-06-23 16:10:23,348 : INFO : resetting layer weights\n",
      "2025-06-23 16:10:23,356 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2025-06-23T16:10:23.356210', 'gensim': '4.3.3', 'python': '3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:29:09) [MSC v.1943 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'build_vocab'}\n",
      "2025-06-23 16:10:23,357 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 4028 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2025-06-23T16:10:23.357210', 'gensim': '4.3.3', 'python': '3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:29:09) [MSC v.1943 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'train'}\n",
      "2025-06-23 16:10:23,675 : INFO : EPOCH 0: training on 188848 raw words (127335 effective words) took 0.3s, 413910 effective words/s\n",
      "2025-06-23 16:10:24,038 : INFO : EPOCH 1: training on 188848 raw words (127030 effective words) took 0.4s, 359402 effective words/s\n",
      "2025-06-23 16:10:24,426 : INFO : EPOCH 2: training on 188848 raw words (127504 effective words) took 0.4s, 337259 effective words/s\n",
      "2025-06-23 16:10:24,804 : INFO : EPOCH 3: training on 188848 raw words (127461 effective words) took 0.4s, 344933 effective words/s\n",
      "2025-06-23 16:10:25,242 : INFO : EPOCH 4: training on 188848 raw words (127498 effective words) took 0.4s, 296488 effective words/s\n",
      "2025-06-23 16:10:25,243 : INFO : Word2Vec lifecycle event {'msg': 'training on 944240 raw words (636828 effective words) took 1.9s, 337907 effective words/s', 'datetime': '2025-06-23T16:10:25.243177', 'gensim': '4.3.3', 'python': '3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:29:09) [MSC v.1943 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'train'}\n",
      "2025-06-23 16:10:25,243 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=4028, vector_size=300, alpha=0.025>', 'datetime': '2025-06-23T16:10:25.243704', 'gensim': '4.3.3', 'python': '3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:29:09) [MSC v.1943 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'created'}\n",
      "2025-06-23 16:10:25,245 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'word2vec_skipgram.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2025-06-23T16:10:25.245318', 'gensim': '4.3.3', 'python': '3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:29:09) [MSC v.1943 64 bit (AMD64)]', 'platform': 'Windows-11-10.0.26100-SP0', 'event': 'saving'}\n",
      "2025-06-23 16:10:25,246 : INFO : not storing attribute cum_table\n",
      "2025-06-23 16:10:25,262 : INFO : saved word2vec_skipgram.model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== 模型参数 =====\n",
      "模型架构: Skip-Gram\n",
      "词表大小: 4028\n",
      "训练总词数: 188848\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 词向量训练（Skip-Gram模式）\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import logging  # 添加日志记录\n",
    "\n",
    "# 配置日志输出\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# 1. 数据预处理\n",
    "def preprocess_text(text):\n",
    "    \"\"\"文本清洗和分词处理\"\"\"\n",
    "    # 去除标点符号（扩展更全的标点集合）\n",
    "    punctuation = \"，。！？、；：“”‘’【】（）《》~@#￥%……&*\"\n",
    "    for p in punctuation:\n",
    "        text = text.replace(p, \"\")\n",
    "    return jieba.lcut(text)\n",
    "\n",
    "# 读入训练集文件\n",
    "data = pd.read_csv('train.csv')\n",
    "corpus = [preprocess_text(str(comment)) for comment in data['comment'].values]\n",
    "\n",
    "# 2. Skip-Gram模型训练\n",
    "model = Word2Vec(\n",
    "    corpus,\n",
    "    sg=1,  # 关键修改：sg=1表示使用Skip-Gram（默认CBOW是sg=0）\n",
    "    vector_size=300,  # 词向量维度\n",
    "    window=5,        # 上下文窗口大小（Skip-Gram通常用更大窗口）\n",
    "    min_count=3,     # 忽略低频词\n",
    "    workers=4,       # 并行线程数\n",
    "    negative=5,      # 负采样数（Skip-Gram推荐5-20）\n",
    "    hs=0,            # 禁用层次softmax（与negative采样二选一）\n",
    "    alpha=0.025,     # 初始学习率\n",
    "    min_alpha=0.0001 # 最小学习率\n",
    ")\n",
    "\n",
    "# 3. 模型保存与加载\n",
    "model.save(\"word2vec_skipgram.model\")  # 保存模型\n",
    "# model = Word2Vec.load(\"word2vec_skipgram.model\")  # 加载模型\n",
    "\n",
    "# 4. 模型验证\n",
    "print('\\n===== 模型参数 =====')\n",
    "print(f\"模型架构: {'Skip-Gram' if model.sg else 'CBOW'}\")\n",
    "print(f\"词表大小: {len(model.wv)}\")\n",
    "print(f\"训练总词数: {model.corpus_total_words}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8988a179-58d7-4c86-bfbb-d39c95ebdf1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "与'点赞'最相似的词：[('人超', 0.9842818975448608), ('帅', 0.9838330149650574), ('耿直', 0.9832215309143066)]\n",
      "与'不错'最相似的词：[('挺不错', 0.9037325978279114), ('齐全', 0.8998403549194336), ('嗯', 0.8991395831108093)]\n",
      "与'难吃'最相似的词：[('咸', 0.8957402110099792), ('垃圾', 0.8782604336738586), ('简直', 0.8750356435775757)]\n",
      "与'推荐'最相似的词：[('值得', 0.8852267265319824), ('一试', 0.880580723285675), ('一去', 0.8680894374847412)]\n",
      "与'地道'最相似的词：[('很赞', 0.98487788438797), ('正宗', 0.9749028086662292), ('很香', 0.969668447971344)]\n",
      "\n",
      "'地道'的词向量（前10维）:\n",
      "[-0.01735038  0.0398961   0.01903399  0.02288429 -0.03887833 -0.0492439\n",
      "  0.05986579  0.3933846  -0.04652843 -0.08235199]\n"
     ]
    }
   ],
   "source": [
    "# 语义相似度查询\n",
    "test_words = ['点赞', '不错', '难吃', '推荐', '地道']\n",
    "for word in test_words:\n",
    "    if word in model.wv:\n",
    "        print(f\"与'{word}'最相似的词：{model.wv.most_similar(word, topn=3)}\")\n",
    "\n",
    "# 向量获取示例\n",
    "if '地道' in model.wv:\n",
    "    print(f\"\\n'地道'的词向量（前10维）:\\n{model.wv['地道'][:10]}\")\n",
    "else:\n",
    "    print(\"\\n警告：'地道'不在词表中\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b47c61cc-8710-4eee-b846-2f81be9d6945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'环境'的词向量（前5维）:\n",
      "[-0.02304971  0.10635543 -0.04023348 -0.00408974 -0.03905172]\n",
      "词向量形状: (300,)\n"
     ]
    }
   ],
   "source": [
    "# 检查并输出\"环境\"的词向量及形状\n",
    "if '环境' in model.wv:\n",
    "    env_vector = model.wv['环境']\n",
    "    print(f\"'环境'的词向量（前5维）:\\n{env_vector[:5]}\")\n",
    "    print(f\"词向量形状: {env_vector.shape}\")  # 应输出 (300,)\n",
    "else:\n",
    "    print(\"警告：'环境'不在词表中\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9a88d47-f5b1-4ac3-ab25-f9a6d92154a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "与'好吃'最相似的3个词:\n",
      "入味: 0.8613\n",
      "棒: 0.8519\n",
      "油腻: 0.8326\n",
      "\n",
      "词语相似度:\n",
      "'好吃' vs '美味': 0.8244\n",
      "'好吃' vs '蟑螂': 0.2673\n"
     ]
    }
   ],
   "source": [
    "# 输出与\"好吃\"最相似的3个词\n",
    "if '好吃' in model.wv:\n",
    "    print(\"\\n与'好吃'最相似的3个词:\")\n",
    "    for word, similarity in model.wv.most_similar('好吃', topn=3):\n",
    "        print(f\"{word}: {similarity:.4f}\")\n",
    "else:\n",
    "    print(\"警告：'好吃'不在词表中\")\n",
    "\n",
    "# 计算词语相似度\n",
    "similarity_results = []\n",
    "for word in ['美味', '蟑螂']:\n",
    "    if '好吃' in model.wv and word in model.wv:\n",
    "        sim = model.wv.similarity('好吃', word)\n",
    "        similarity_results.append((word, sim))\n",
    "    else:\n",
    "        print(f\"警告：'{word}'不在词表中\")\n",
    "\n",
    "print(\"\\n词语相似度:\")\n",
    "for word, sim in similarity_results:\n",
    "    print(f\"'好吃' vs '{word}': {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9eceec0-25f5-4f19-bf5b-bea9dc9e2c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "向量运算 '餐厅 + 聚会 - 安静' ≈ '家庭聚会' (相似度: 0.9598)\n"
     ]
    }
   ],
   "source": [
    "# 向量类比计算\n",
    "if all(word in model.wv for word in ['餐厅', '聚会', '安静']):\n",
    "    result = model.wv.most_similar(\n",
    "        positive=['餐厅', '聚会'],\n",
    "        negative=['安静'],\n",
    "        topn=1\n",
    "    )\n",
    "    print(f\"\\n向量运算 '餐厅 + 聚会 - 安静' ≈ '{result[0][0]}' (相似度: {result[0][1]:.4f})\")\n",
    "else:\n",
    "    print(\"警告：计算所需的词未全部存在于词表中\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
